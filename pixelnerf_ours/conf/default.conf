# Single-view only base model
# (Not used in experiments; resnet_fine_mv.conf inherits)
model {
    use_encoder = True # Condition on local encoder    
    use_global_encoder = False # Condition also on a global encoder? 
    use_xyz = True # Use xyz input instead of just z   
    canon_xyz = False # Canonical space xyz (default view space)    
    use_code = False # Positional encoding
    
    code {
        num_freqs = 0 
        freq_factor = 0
        include_input = True
    }

    use_viewdirs = True # View directions
    use_code_viewdirs = False # Apply pos. enc. to viewdirs?

    mlp_coarse { # MLP architecture
        type = resnet  # Can change to mlp
        d_hidden = 512
        n_blocks = 3        
    }
    
    mlp_fine {
        type = resnet
        d_hidden = 512
        n_blocks = 3        
    }

    encoder { # Encoder architecture
        backbone = resnet34
        pretrained = True
        num_layers = 4
    }
}

renderer {
    n_coarse = 64
    n_fine = 128
    n_fine_depth = 64 # Try using expected depth sample
    depth_std = 0.01  # Noise to add to depth sample
    sched = [] # Decay schedule, not used    
    white_bkgd = True # White background color (false : black)
}

loss {
    rgb { # RGB losses coarse/fine
        use_l1 = False
    }
    rgb_fine {
        use_l1 = False
    }
    
    alpha { # Alpha regularization (disabled in final version)
        lambda_alpha = 0.0 # lambda_alpha = 0.0001
        clamp_alpha = 100
        init_epoch = 5
    }
    # Coarse/fine weighting (nerf = equal)
    lambda_coarse = 1.0  # loss = lambda_coarse * loss_coarse + loss_fine
    lambda_fine = 1.0  # loss = lambda_coarse * loss_coarse + loss_fine
}

train {
    # Training 
    print_interval = 200
    save_interval = 200
    vis_interval = 1000
    eval_interval = 20

    # Accumulating gradients. Not really recommended.
    # 1 = disable
    accu_grad = 1

    # Number of times to repeat dataset per 'epoch'
    # Useful if dataset is extremely small, like DTU
    num_epoch_repeats = 32
}